{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéì **Professor**: Apostolos Filippas\n",
    "\n",
    "### üìò **Class**: AI Engineering\n",
    "\n",
    "### üìã **Topic**: You Can Just Build Things\n",
    "\n",
    "üö´ **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In our firstfour lectures, we've covered how\n",
    "1. We can call LLMs via APIs and get structured responses\n",
    "2. We can build lexical search with BM25\n",
    "3. We can build semantic search with embeddings\n",
    "4. We can combine lexical and semantic search into hybrid search\n",
    "\n",
    "Today you will put it all together by building a Retrieval Augmented Generation (RAG) system.\n",
    "- This is a question-answering bot that can answer questions about Fordham University\n",
    "- You will use real data scraped from the Fordham website.\n",
    "\n",
    "\n",
    "Your RAG pipeline will look like this:\n",
    "\n",
    "```\n",
    "User Question\n",
    "     ‚Üì\n",
    "1. RETRIEVE: Find relevant documents (search!)\n",
    "     ‚Üì\n",
    "2. AUGMENT: Stuff those documents into a prompt\n",
    "     ‚Üì\n",
    "3. GENERATE: Ask an LLM to answer using the context\n",
    "     ‚Üì\n",
    "Answer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Look at your data\n",
    "\n",
    "In `data/fordham-website.zip` you'll find **~9,500 Markdown files** scraped from Fordham's website. Each file is one page ‚Äî admissions info, program descriptions, faculty pages, financial aid, campus life, and more.\n",
    "\n",
    "Your task: **look at the data**\n",
    "- The first step in any AI engineering or data science project should always be to familiarize yourself with the data.\n",
    "- I cannot stress this enough.. without this step, it's hard to build anything useful.\n",
    "\n",
    "Tips:\n",
    "- Unzip the archive and look at some of the files. \n",
    "- Open a few in a text editor. \n",
    "- Get a feel for what you're working with.\n",
    "- The first line of every file is always the **URL** of the page it was scraped from. The rest is the page content converted to Markdown. Here's an example ‚Äî `gabelli-school-of-business_veterans.md`:\n",
    "\n",
    "```markdown\n",
    "https://www.fordham.edu/gabelli-school-of-business/veterans\n",
    "\n",
    "# Military Veterans & Active Duty Members of the Military\n",
    "\n",
    "## Transform Your Knowledge & Skills Into a Business Career for the Future\n",
    "\n",
    "As a veteran or an active duty member of the United States Armed Services,\n",
    "you have gained or are currently acquiring the invaluable organizational,\n",
    "leadership, analytics, and technical knowledge and skills that hiring\n",
    "managers seek. These transferrable skills provide a major advantage in\n",
    "emerging, business-related industries where innovation, a global mind-set,\n",
    "and the ability to lead individuals and teams in the continuously evolving\n",
    "work environment, are critical for success.\n",
    "\n",
    "By completing a graduate or undergraduate business degree at the Gabelli\n",
    "School of Business, you can prepare for a lifelong career in some of\n",
    "today's fastest-growing fields. ...\n",
    "\n",
    "### Study at a Top-Ranked, Military-Friendly University\n",
    "\n",
    "The Gabelli School of Business is part of Fordham University, the only\n",
    "New York City university to be among those ranked \"Best for Vets\" by\n",
    "Military Times. ...\n",
    "\n",
    "### Learn How the Yellow Ribbon Program Works\n",
    "\n",
    "The Yellow Ribbon GI Education Enhancement Program, or the Yellow Ribbon\n",
    "Program, is a part of the Post-9/11 Veterans Educational Assistance Act\n",
    "of 2008. ...\n",
    "```\n",
    "\n",
    "The filenames mirror the URL structure ‚Äî underscores replace path separators (e.g. `gabelli-school-of-business_veterans.md` came from `/gabelli-school-of-business/veterans`). Some files are short (a few lines), others are quite long.\n",
    "\n",
    "- Once you've looked around, load the files into Python. Python's built-in `zipfile` module can read zip archives without extracting to disk. Load them into a list of dictionaries or a DataFrame with at least two fields: the filename (or a clean page name) and the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for your implementation\n",
    "# Core\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing / retrieval\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Embeddings (OpenAI example ‚Äì adapt to your provider)\n",
    "from openai import OpenAI\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings (cheap and good)\n",
    "EMBED_MODEL = \"text-embedding-3-small\"  # Keep this - works great with 4o-mini\n",
    "\n",
    "# Generation (your preferred model)\n",
    "GEN_MODEL = \"gpt-4o-mini\"  # ‚úÖ Your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9560 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d680e8a854a7cbad6d490c445cba2eba.md</td>\n",
       "      <td>https://www.fordham.edu/d680e8a854a7cbad6d490c...</td>\n",
       "      <td>index.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42f623b3bad309d5d6619d450af47d40.md</td>\n",
       "      <td>https://www.fordham.edu/42f623b3bad309d5d6619d...</td>\n",
       "      <td>research.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbcc2bf9adc75ea7fce66bd4eb246203.md</td>\n",
       "      <td>https://www.fordham.edu/cbcc2bf9adc75ea7fce66b...</td>\n",
       "      <td>ccel.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>717c9560db9ebed56c22664925c4c1e6.md</td>\n",
       "      <td>https://www.fordham.edu/717c9560db9ebed56c2266...</td>\n",
       "      <td>fordham-college-at-lincoln-center.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d8a2a1a177f439f2ca185e084642f713.md</td>\n",
       "      <td>https://www.fordham.edu/d8a2a1a177f439f2ca185e...</td>\n",
       "      <td>academics.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename  \\\n",
       "0  d680e8a854a7cbad6d490c445cba2eba.md   \n",
       "1  42f623b3bad309d5d6619d450af47d40.md   \n",
       "2  cbcc2bf9adc75ea7fce66bd4eb246203.md   \n",
       "3  717c9560db9ebed56c22664925c4c1e6.md   \n",
       "4  d8a2a1a177f439f2ca185e084642f713.md   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.fordham.edu/d680e8a854a7cbad6d490c...   \n",
       "1  https://www.fordham.edu/42f623b3bad309d5d6619d...   \n",
       "2  https://www.fordham.edu/cbcc2bf9adc75ea7fce66b...   \n",
       "3  https://www.fordham.edu/717c9560db9ebed56c2266...   \n",
       "4  https://www.fordham.edu/d8a2a1a177f439f2ca185e...   \n",
       "\n",
       "                                content  \n",
       "0                              index.md  \n",
       "1                           research.md  \n",
       "2                               ccel.md  \n",
       "3  fordham-college-at-lincoln-center.md  \n",
       "4                          academics.md  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_JSON_PATH = Path(\"fordham-website-windows.json\")  # Your uploaded file\n",
    "\n",
    "def load_json_to_df(json_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load your JSON file into a DataFrame.\n",
    "    Assumes it's a list of dicts with 'filename', 'url', 'content' \n",
    "    OR a dict with keys=filenames, values=content.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        # List of dicts format\n",
    "        rows = data\n",
    "    elif isinstance(data, dict):\n",
    "        # Dict format: {\"filename1\": \"content1\", \"filename2\": \"content2\"}\n",
    "        rows = []\n",
    "        for filename, content in data.items():\n",
    "            # Try to extract URL from filename (common pattern)\n",
    "            url = f\"https://www.fordham.edu/{filename.replace('.md', '').replace('_', '/')}/\"\n",
    "            rows.append({\"filename\": filename, \"url\": url, \"content\": content})\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected JSON format\")\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "docs_df = load_json_to_df(DATA_JSON_PATH)\n",
    "print(f\"Loaded {len(docs_df)} documents\")\n",
    "docs_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON...\n",
      "‚úÖ Loaded 9560 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "DATA_JSON_PATH = Path(\"fordham-website-windows.json\")\n",
    "print(\"Loading JSON...\")\n",
    "\n",
    "def load_json_to_df(json_path: Path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # Handle dict format {filename: content}\n",
    "    if isinstance(data, dict):\n",
    "        rows = [{\"filename\": k, \"url\": f\"https://fordham.edu/{k.replace('.md','').replace('_','/')}\", \"content\": v} \n",
    "                for k, v in data.items()]\n",
    "    else:\n",
    "        rows = data  # List format\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "docs_df = load_json_to_df(DATA_JSON_PATH)\n",
    "print(f\"‚úÖ Loaded {len(docs_df)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Chunk the Documents\n",
    "\n",
    "Some of the pages could be too long to embed as a single unit. Down the line, the pages may be too long to stuff into the LLM's prompt during the generation step. As such, most of the RAG systems will break down big documents into into smaller **chunks**.\n",
    "\n",
    "> üìö **TERM: Chunking**  \n",
    "> Splitting documents into smaller, self-contained pieces for embedding and retrieval. The goal is chunks that are small enough to be specific, but large enough to be meaningful.\n",
    "\n",
    "Your task: **write a function that splits each document into chunks.**\n",
    "\n",
    "Things to think about:\n",
    "- What's a reasonable chunk size? (Think about what fits in a prompt vs. what's too vague)\n",
    "- Should you split on sentences? Paragraphs? A fixed character/word count?\n",
    "- Should chunks overlap? What happens if an answer spans two chunks?\n",
    "- How do you keep track of which document each chunk came from? You may need that information down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Manikandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Manikandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK punkt downloaded!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # Extra safety for some versions\n",
    "print(\"‚úÖ NLTK punkt downloaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON type: <class 'dict'>\n",
      "JSON keys/len: 9560\n",
      "\n",
      "First 3 keys/items:\n",
      "  d680e8a854a7cbad6d490c445cba2eba.md: index.md\n",
      "  Content len: 8\n",
      "  42f623b3bad309d5d6619d450af47d40.md: research.md\n",
      "  Content len: 11\n",
      "  cbcc2bf9adc75ea7fce66bd4eb246203.md: ccel.md\n",
      "  Content len: 7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_JSON_PATH = Path(\"fordham-website-windows.json\")\n",
    "\n",
    "# Peek at the data structure\n",
    "with open(DATA_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"JSON type:\", type(data))\n",
    "print(\"JSON keys/len:\", len(data) if isinstance(data, dict) else len(data))\n",
    "print(\"\\nFirst 3 keys/items:\")\n",
    "if isinstance(data, dict):\n",
    "    keys = list(data.keys())[:3]\n",
    "    for k in keys:\n",
    "        content_start = str(data[k])[:200] + \"...\" if len(str(data[k])) > 200 else str(data[k])\n",
    "        print(f\"  {k}: {content_start}\")\n",
    "        print(f\"  Content len: {len(str(data[k]))}\")\n",
    "elif isinstance(data, list):\n",
    "    for i, item in enumerate(data[:3]):\n",
    "        print(f\"  Item {i}: {item}\")\n",
    "else:\n",
    "    print(\"Unexpected format:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning fordham-website-windows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c18f76698a4bde8ba02cb642345098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading .md files:   0%|          | 0/9560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 9560 valid .md documents\n",
      "\n",
      "Sample:\n",
      "                              filename  \\\n",
      "0  0001144e6d954f94682637e541ad5d7f.md   \n",
      "1  000121f75daed3fee3eb14cdb934a788.md   \n",
      "2  001fb84f7d97bdd24d99b581d920a602.md   \n",
      "3  002239821b73a238c5b698e20708f3e5.md   \n",
      "4  002a8f1a0801c409834b8b47158f33d8.md   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.fordham.edu/about/living-the-missi...  \n",
      "1  https://www.fordham.edu/academics/departments/...  \n",
      "2  https://www.fordham.edu/information-technology...  \n",
      "3  https://www.fordham.edu/summer-session/pre-col...  \n",
      "4  https://www.fordham.edu/school-of-professional...  \n",
      "\n",
      "Chunking 9560 docs in batches of 500...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25ca33c92a14d7cb18406b7c9ffe0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 1827 chunks, Total so far: 1827\n",
      "Batch 2: 2430 chunks, Total so far: 4257\n",
      "Batch 3: 1808 chunks, Total so far: 6065\n",
      "Batch 4: 2539 chunks, Total so far: 8604\n",
      "Batch 5: 1616 chunks, Total so far: 10220\n",
      "Batch 6: 2397 chunks, Total so far: 12617\n",
      "Batch 7: 1708 chunks, Total so far: 14325\n",
      "Batch 8: 1543 chunks, Total so far: 15868\n",
      "Batch 9: 1910 chunks, Total so far: 17778\n",
      "Batch 10: 1895 chunks, Total so far: 19673\n",
      "Batch 11: 1725 chunks, Total so far: 21398\n",
      "Batch 12: 2183 chunks, Total so far: 23581\n",
      "Batch 13: 2032 chunks, Total so far: 25613\n",
      "Batch 14: 1916 chunks, Total so far: 27529\n",
      "Batch 15: 1455 chunks, Total so far: 28984\n",
      "Batch 16: 1944 chunks, Total so far: 30928\n",
      "Batch 17: 1883 chunks, Total so far: 32811\n",
      "Batch 18: 1783 chunks, Total so far: 34594\n",
      "Batch 19: 2056 chunks, Total so far: 36650\n",
      "Batch 20: 203 chunks, Total so far: 36853\n",
      "\n",
      "‚úÖ FINAL: 36853 chunks saved!\n",
      "                              filename  \\\n",
      "0  0001144e6d954f94682637e541ad5d7f.md   \n",
      "1  0001144e6d954f94682637e541ad5d7f.md   \n",
      "2  000121f75daed3fee3eb14cdb934a788.md   \n",
      "3  000121f75daed3fee3eb14cdb934a788.md   \n",
      "4  001fb84f7d97bdd24d99b581d920a602.md   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.fordham.edu/about/living-the-missi...   \n",
      "1  https://www.fordham.edu/about/living-the-missi...   \n",
      "2  https://www.fordham.edu/academics/departments/...   \n",
      "3  https://www.fordham.edu/academics/departments/...   \n",
      "4  https://www.fordham.edu/information-technology...   \n",
      "\n",
      "                                                text  \n",
      "0  # 2021-2022 Duffy Fellows\\n\\n**Afrah Bandagi (...  \n",
      "1  and Migration Activism‚Äù (research presentation...  \n",
      "2  # Medieval Studies MA Admission and Financial ...  \n",
      "3  on with GSAS funding consideration is **Januar...  \n",
      "4  # Speechify\\n\\n**For Students, Faculty, Staff*...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# STEP 1+2: Load .md files from folder + chunk (memory safe)\n",
    "FOLDER_PATH = Path(\"fordham-website-windows\")\n",
    "print(f\"Scanning {FOLDER_PATH}...\")\n",
    "\n",
    "# 1. LOAD ALL .md FILES FROM FOLDER\n",
    "docs_list = []\n",
    "for md_file in tqdm(list(FOLDER_PATH.glob(\"*.md\")), desc=\"Loading .md files\"):\n",
    "    try:\n",
    "        with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            raw_content = f.read()\n",
    "        \n",
    "        # First line = URL, rest = content (per your notebook spec)\n",
    "        lines = raw_content.strip().split('\\n')\n",
    "        if lines:\n",
    "            url = lines[0].strip()\n",
    "            content = '\\n'.join(lines[1:]).strip()\n",
    "            \n",
    "            if len(content) > 50:  # Skip tiny files\n",
    "                docs_list.append({\n",
    "                    \"filename\": md_file.name,\n",
    "                    \"url\": url,\n",
    "                    \"content\": content,\n",
    "                    \"file_path\": str(md_file)\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {md_file.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "docs_df = pd.DataFrame(docs_list)\n",
    "print(f\"‚úÖ Loaded {len(docs_df)} valid .md documents\")\n",
    "print(\"\\nSample:\")\n",
    "print(docs_df[[\"filename\", \"url\"]].head())\n",
    "\n",
    "# 2. STREAMING CHUNKER (same as before)\n",
    "def chunk_doc_streaming(text: str, source_id: str, url: str, filename: str):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_size = 2000\n",
    "    overlap = 200\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            end = text.find('. ', start + chunk_size//2, end)\n",
    "            if end == -1: \n",
    "                end = start + chunk_size\n",
    "        \n",
    "        chunk = text[start:end].strip()\n",
    "        if len(chunk) > 100:\n",
    "            chunks.append({\n",
    "                \"source_id\": source_id, \n",
    "                \"url\": url, \n",
    "                \"filename\": filename,\n",
    "                \"chunk_id\": len(chunks), \n",
    "                \"text\": chunk\n",
    "            })\n",
    "        start = max(end - overlap, 0)\n",
    "        if start >= len(text): \n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# Process 500 docs at a time (memory safe)\n",
    "BATCH_SIZE_DOCS = 500\n",
    "chunks_all = []\n",
    "\n",
    "print(f\"\\nChunking {len(docs_df)} docs in batches of {BATCH_SIZE_DOCS}...\")\n",
    "for batch_start in tqdm(range(0, len(docs_df), BATCH_SIZE_DOCS)):\n",
    "    batch_end = min(batch_start + BATCH_SIZE_DOCS, len(docs_df))\n",
    "    batch_docs = docs_df.iloc[batch_start:batch_end]\n",
    "    \n",
    "    batch_chunks = []\n",
    "    for idx, row in batch_docs.iterrows():\n",
    "        doc_chunks = chunk_doc_streaming(\n",
    "            row[\"content\"], str(idx), row[\"url\"], row[\"filename\"]\n",
    "        )\n",
    "        batch_chunks.extend(doc_chunks)\n",
    "    \n",
    "    chunks_all.extend(batch_chunks)\n",
    "    print(f\"Batch {batch_start//BATCH_SIZE_DOCS + 1}: {len(batch_chunks)} chunks, \"\n",
    "          f\"Total so far: {len(chunks_all)}\")\n",
    "    \n",
    "    del batch_docs, batch_chunks\n",
    "    gc.collect()\n",
    "\n",
    "# Final save\n",
    "chunks_df = pd.DataFrame(chunks_all)\n",
    "chunks_df.to_parquet(\"fordham_chunks_final.parquet\")\n",
    "print(f\"\\n‚úÖ FINAL: {len(chunks_df)} chunks saved!\")\n",
    "print(chunks_df[[\"filename\", \"url\", \"text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Embed the Chunks\n",
    "\n",
    "Now we need to turn each chunk into a vector so we can search over them. You've done this before in Lecture 4.\n",
    "\n",
    "Your task: **embed all chunks using an embedding model.**\n",
    "\n",
    "Tips:\n",
    "- You could use a local model, or API model. What are the tradeoffs?\n",
    "- This will take a while if you do it serially. You might want to use async/batch.\n",
    "- Once you've created your embeddings, you may want to save them to disk so you don't have to redo this step every time\n",
    "- You'll need to embed queries with the **same model** at search time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Embedding 36853 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5946e48d8006449c9cc192683a112f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved (36853, 1536)\n",
      "‚úÖ Embeddings ready: (36853, 1536)\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for your implementation\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "def batch_embed(texts: list, batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = [t[:4000] for t in texts[i:i + batch_size]]  # Truncate\n",
    "        response = client.embeddings.create(model=EMBED_MODEL, input=batch_texts)\n",
    "        batch_embs = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embs)\n",
    "    return np.array(all_embeddings, dtype=\"float32\")\n",
    "\n",
    "# Load chunks (if restarted)\n",
    "chunks_df = pd.read_parquet(\"fordham_chunks_final.parquet\")\n",
    "\n",
    "# Embed (cached)\n",
    "EMB_PATH = Path(\"fordham_embeddings.npy\")\n",
    "if EMB_PATH.exists():\n",
    "    print(\"‚úÖ Loading cached embeddings...\")\n",
    "    chunk_embeddings = np.load(EMB_PATH)\n",
    "else:\n",
    "    print(f\"üîÑ Embedding {len(chunks_df)} chunks...\")\n",
    "    texts = chunks_df[\"text\"].tolist()\n",
    "    chunk_embeddings = batch_embed(texts)\n",
    "    np.save(EMB_PATH, chunk_embeddings)\n",
    "    print(f\"‚úÖ Saved {chunk_embeddings.shape}\")\n",
    "\n",
    "print(f\"‚úÖ Embeddings ready: {chunk_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Retrieve\n",
    "\n",
    "Now build the **R** in RAG. Given a user's question, find the most relevant chunks.\n",
    "\n",
    "Your task: **write a retrieval function that takes a question and returns the most relevant chunks.**\n",
    "\n",
    "Tips:\n",
    "- You can use lexical or semantic search or both!\n",
    "- How many chunks should you retrieve? Too few and you might miss the answer; too many and you'll overwhelm the LLM (and pay more tokens)\n",
    "- Try a few test questions and eyeball whether the retrieved chunks are relevant\n",
    "- Try a few questions and see what comes back. For example:\n",
    "  - \"What programs does the Gabelli School of Business offer?\"\n",
    "  - \"How do I apply for financial aid?\"\n",
    "  - \"Where is Fordham's campus?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 32) (2411141498.py, line 32)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mAnswer briefly using only the context above. If not in context, say \"Not found in Fordham docs.\"\"\"\"\u001b[39m\n                                                                                                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 32)\n"
     ]
    }
   ],
   "source": [
    "# Your implementation here\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=query[:4000])\n",
    "    return np.array(resp.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "def retrieve_top_k(query: str, k: int = 6) -> pd.DataFrame:\n",
    "    q_emb = embed_query(query)\n",
    "    sims = cosine_similarity(q_emb.reshape(1, -1), chunk_embeddings)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:k]\n",
    "    results = chunks_df.iloc[top_idx].copy()\n",
    "    results[\"score\"] = sims[top_idx]\n",
    "    return results.sort_values(\"score\", ascending=False)\n",
    "\n",
    "GEN_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "def rag(question: str, k: int = 6) -> dict:\n",
    "    retrieved = retrieve_top_k(question, k)\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"[Source: {row['url']}]\\n{row['text'][:1000]}...\" \n",
    "        for _, row in retrieved.iterrows()\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Using ONLY this Fordham University context, answer the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer briefly using only the context above. If not in context, say \"Not found in Fordham docs.\"\"\"\"\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=GEN_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": resp.choices[0].message.content.strip(),\n",
    "        \"sources\": retrieved[[\"url\", \"filename\", \"score\"]].to_dict(\"records\")\n",
    "    }\n",
    "\n",
    "# TEST IT!\n",
    "result = rag(\"What programs does Gabelli School offer?\")\n",
    "print(\"Q:\", result[\"question\"])\n",
    "print(\"A:\", result[\"answer\"])\n",
    "print(\"Sources:\", [s[\"url\"] for s in result[\"sources\"][:2]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> np.ndarray:\n",
    "    return np.array(get_embedding(query), dtype=\"float32\")\n",
    "\n",
    "def retrieve_semantic(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    q_emb = embed_query(query)\n",
    "    sims = cosine_similarity(\n",
    "        q_emb.reshape(1, -1),\n",
    "        chunk_embeddings\n",
    "    )[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:k]\n",
    "    results = chunks_df.iloc[top_idx].copy()\n",
    "    results[\"score\"] = sims[top_idx]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fit TF-IDF on chunks once\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tfidf = \u001b[43mTfidfVectorizer\u001b[49m(max_features=\u001b[32m50000\u001b[39m)\n\u001b[32m      3\u001b[39m tfidf_matrix = tfidf.fit_transform(chunks_df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_hybrid\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m      6\u001b[39m                     alpha: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m) -> pd.DataFrame:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# alpha: weight for semantic, (1-alpha) for lexical\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit TF-IDF on chunks once\n",
    "tfidf = TfidfVectorizer(max_features=50000)\n",
    "tfidf_matrix = tfidf.fit_transform(chunks_df[\"text\"].tolist())\n",
    "\n",
    "def retrieve_hybrid(query: str, k: int = 5,\n",
    "                    alpha: float = 0.5) -> pd.DataFrame:\n",
    "    # alpha: weight for semantic, (1-alpha) for lexical\n",
    "    q_emb = embed_query(query)\n",
    "    sem_sims = cosine_similarity(\n",
    "        q_emb.reshape(1, -1),\n",
    "        chunk_embeddings\n",
    "    )[0]\n",
    "\n",
    "    q_tfidf = tfidf.transform([query])\n",
    "    lex_sims = cosine_similarity(q_tfidf, tfidf_matrix)[0]\n",
    "\n",
    "    combined = alpha * sem_sims + (1 - alpha) * lex_sims\n",
    "    top_idx = np.argsort(combined)[::-1][:k]\n",
    "    res = chunks_df.iloc[top_idx].copy()\n",
    "    res[\"semantic_score\"] = sem_sims[top_idx]\n",
    "    res[\"lexical_score\"] = lex_sims[top_idx]\n",
    "    res[\"score\"] = combined[top_idx]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What programs does Gabelli School offer?\n",
      "\n",
      "A: The Gabelli School offers three types of M.B.A. programs (full-time, professional, and executive M.B.A.), 12 Master of Science programs (two offered online), and two doctoral programs (Ph.D. and Doctor of Professional Studies).\n",
      "\n",
      "Sources: ['https://www.fordham.edu/gabelli-school-of-business/academic-programs-and-admissions', 'https://www.fordham.edu/gabelli-school-of-business/academic-programs-and-admissions/graduate-programs/academic-programs']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=query[:4000])\n",
    "    return np.array(resp.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "def retrieve_top_k(query: str, k: int = 6) -> pd.DataFrame:\n",
    "    q_emb = embed_query(query)\n",
    "    sims = cosine_similarity(q_emb.reshape(1, -1), chunk_embeddings)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:k]\n",
    "    results = chunks_df.iloc[top_idx].copy()\n",
    "    results[\"score\"] = sims[top_idx]\n",
    "    return results.sort_values(\"score\", ascending=False)\n",
    "\n",
    "GEN_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "def rag(question: str, k: int = 6) -> dict:\n",
    "    retrieved = retrieve_top_k(question, k)\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"[Source: {row['url']}]\\n{row['text'][:1000]}...\" \n",
    "        for _, row in retrieved.iterrows()\n",
    "    ])\n",
    "    \n",
    "    # FIXED: Proper triple quotes\n",
    "    prompt = \"\"\"Using ONLY this Fordham University context, answer the question.\n",
    "\n",
    "CONTEXT:\n",
    "{}\n",
    "QUESTION: {}\n",
    "\n",
    "Answer briefly using only the context above. If not in context, say \"Not found in Fordham docs.\" \"\"\".format(context, question)\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=GEN_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": resp.choices[0].message.content.strip(),\n",
    "        \"sources\": retrieved[[\"url\", \"filename\", \"score\"]].to_dict(\"records\")\n",
    "    }\n",
    "\n",
    "# TEST IT!\n",
    "result = rag(\"What programs does Gabelli School offer?\")\n",
    "print(\"Q:\", result[\"question\"])\n",
    "print(\"\\nA:\", result[\"answer\"])\n",
    "print(\"\\nSources:\", [s[\"url\"] for s in result[\"sources\"][:2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Generate\n",
    "\n",
    "Now build the **G** in RAG. Take the retrieved chunks and pass them to an LLM along with the user's question.\n",
    "\n",
    "Your task: **write a function that takes a question and the retrieved chunks, builds a prompt, and calls an LLM to generate an answer.**\n",
    "\n",
    "Tips:\n",
    "- How should you structure the prompt? The LLM needs to know: (1) what is the context of the application, (2) what is the question, (3) what it should include in its answer\n",
    "- What should the LLM do if the context doesn't contain the answer?\n",
    "- Start with a cheap model; try a better one when you've figured out the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "Fordham University has two campuses in New York City: Rose Hill in the Bronx and Lincoln Center in Manhattan.\n"
     ]
    }
   ],
   "source": [
    "GEN_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "def build_context(retrieved_chunks: pd.DataFrame) -> str:\n",
    "    \"\"\"Build context string from retrieved chunks\"\"\"\n",
    "    context_parts = []\n",
    "    for _, row in retrieved_chunks.iterrows():\n",
    "        context_parts.append(f\"[Source: {row['url']}]\\n{row['text'][:1000]}...\")\n",
    "    return \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "def generate_answer(question: str, retrieved_chunks: pd.DataFrame) -> str:\n",
    "    \"\"\"Step 5: GENERATE - LLM answers using retrieved context\"\"\"\n",
    "    context = build_context(retrieved_chunks)\n",
    "    \n",
    "    prompt = \"\"\"Using ONLY this Fordham University context, answer the question.\n",
    "\n",
    "CONTEXT:\n",
    "{}\n",
    "QUESTION: {}\n",
    "\n",
    "Answer briefly using only the context above. If not in context, say \"Not found in Fordham docs.\" \"\"\".format(context, question)\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=GEN_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# Test generation\n",
    "question = \"Where is Fordham's campus?\"\n",
    "retrieved = retrieve_top_k(question, k=5)\n",
    "answer = generate_answer(question, retrieved)\n",
    "print(\"Generated Answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Wire everything together\n",
    "\n",
    "Combine the previous steps into a simple function that takes in a question and returns an answer.\n",
    "\n",
    "Your task: **write a `rag(question)` function that retrieves relevant chunks and generates an answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL RAG PIPELINE TEST ===\n",
      "\n",
      "Q: Where is Fordham's campus?\n",
      "A: Fordham University has two campuses in New York City: Rose Hill in the Bronx and Lincoln Center in Manhattan.\n",
      "Sources: 5 pages\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What programs does Gabelli School offer?\n",
      "A: The Gabelli School offers three types of M.B.A. programs (full-time, professional, and executive M.B.A.), 12 Master of Science programs (two offered online), and two doctoral programs (Ph.D. and Doctor of Professional Studies).\n",
      "Sources: 5 pages\n",
      "--------------------------------------------------------------------------------\n",
      "Q: How do I apply for financial aid?\n",
      "A: To apply for financial aid at Fordham University, you need to fill out the Free Application for Federal Student Aid (FAFSA) and, if applicable, the CSS Profile. Ensure you are a U.S. citizen or eligible non-citizen, making satisfactory academic progress, and enrolled as a matriculated student in an approved program. You can monitor your financial aid application status through the financial aid portal and will receive an award offer by email within four weeks of completing your application.\n",
      "Sources: 5 pages\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def rag(question: str, k: int = 6) -> dict:\n",
    "    \"\"\"Step 6: RAG - Retrieve + Augment + Generate\"\"\"\n",
    "    # RETRIEVE\n",
    "    retrieved = retrieve_top_k(question, k)\n",
    "    \n",
    "    # AUGMENT + GENERATE  \n",
    "    answer = generate_answer(question, retrieved)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": retrieved[[\"url\", \"filename\", \"score\"]].to_dict(\"records\")\n",
    "    }\n",
    "\n",
    "# Test complete pipeline\n",
    "test_questions = [\n",
    "    \"Where is Fordham's campus?\",\n",
    "    \"What programs does Gabelli School offer?\",\n",
    "    \"How do I apply for financial aid?\"\n",
    "]\n",
    "\n",
    "print(\"=== FULL RAG PIPELINE TEST ===\\n\")\n",
    "for question in test_questions:\n",
    "    result = rag(question, k=5)\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: {len(result['sources'])} pages\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Evaluate, experiment and improve\n",
    "\n",
    "Your RAG system works ‚Äî but there's always room to make it better. \n",
    "\n",
    "Your task: **evaluate, experiment, and improve your system**\n",
    "\n",
    "Tips:\n",
    "- How do you know that your system is working or that your changes are improving it?\n",
    "- Try different questions ‚Äî where does it do well? Where does it struggle?\n",
    "- Adjust the number of retrieved chunks ‚Äî what happens with more or fewer?\n",
    "- Try different chunking strategies ‚Äî bigger chunks? Smaller? Overlap?\n",
    "- Try a different embedding model ‚Äî does it change retrieval quality?\n",
    "- Improve the prompt ‚Äî can you get better, more concise answers?\n",
    "- Add source attribution ‚Äî can the system tell the user which pages the answer came from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ RAG EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä PERFORMANCE SUMMARY\n",
      "  quality       top_score      \n",
      "     mean count      mean count\n",
      "k                              \n",
      "3     1.0     6  0.684613     6\n",
      "6     1.0     6  0.684610     6\n",
      "\n",
      "üèÜ BEST ANSWERS:\n",
      "Q: What programs does Gabelli School of Business offer?...\n",
      "  Top source score: 0.756\n",
      "  Answer: The Gabelli School of Business offers three variants of the MBA program: a full-...\n",
      "\n",
      "Q: What programs does Gabelli School of Business offer?...\n",
      "  Top source score: 0.756\n",
      "  Answer: The Gabelli School of Business offers three types of MBA programs (full-time, pr...\n",
      "\n",
      "Q: Where is Fordham's campus?...\n",
      "  Top source score: 0.745\n",
      "  Answer: Fordham University has two campuses in New York City: Rose Hill in the Bronx and...\n",
      "\n",
      "\n",
      "üî¨ EXPERIMENT: Impact of K (more context)\n",
      "üß™ RAG EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä PERFORMANCE SUMMARY\n",
      "     quality       top_score      \n",
      "        mean count      mean count\n",
      "k                                 \n",
      "1   0.666667     3  0.704771     3\n",
      "3   1.000000     3  0.704757     3\n",
      "6   1.000000     3  0.704757     3\n",
      "10  1.000000     3  0.704778     3\n",
      "\n",
      "üèÜ BEST ANSWERS:\n",
      "Q: What programs does Gabelli School of Business offer?...\n",
      "  Top source score: 0.756\n",
      "  Answer: The Gabelli School of Business offers three variants of the MBA program: a full-...\n",
      "\n",
      "Q: What programs does Gabelli School of Business offer?...\n",
      "  Top source score: 0.756\n",
      "  Answer: The Gabelli School of Business offers three variants of the MBA program (full-ti...\n",
      "\n",
      "Q: What programs does Gabelli School of Business offer?...\n",
      "  Top source score: 0.756\n",
      "  Answer: The Gabelli School of Business offers three variants of the MBA program: a full-...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for your implementation\n",
    "# Step 7: EVALUATION FRAMEWORK\n",
    "test_questions = [\n",
    "    \"Where is Fordham's campus?\",\n",
    "    \"What programs does Gabelli School of Business offer?\",\n",
    "    \"How do I apply for financial aid?\",\n",
    "    \"What is the Yellow Ribbon Program?\",\n",
    "    \"Gabelli School veterans benefits\",\n",
    "    \"Fordham medieval studies program\"\n",
    "]\n",
    "\n",
    "def evaluate_rag(n_questions=20, k_values=[3, 6, 10], use_hybrid=False):\n",
    "    \"\"\"Manual evaluation + experiments\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"üß™ RAG EVALUATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, question in enumerate(test_questions[:n_questions]):\n",
    "        for k in k_values:\n",
    "            result = rag(question, k=k)\n",
    "            \n",
    "            # Simple quality metrics\n",
    "            score = 1 if len(result[\"answer\"]) > 20 and \"not found\" not in result[\"answer\"].lower() else 0\n",
    "            relevance = result[\"sources\"][0][\"score\"] if result[\"sources\"] else 0\n",
    "            \n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"k\": k,\n",
    "                \"answer\": result[\"answer\"][:100] + \"...\",\n",
    "                \"top_source\": result[\"sources\"][0][\"url\"] if result[\"sources\"] else \"None\",\n",
    "                \"top_score\": relevance,\n",
    "                \"quality\": score\n",
    "            })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Summary stats\n",
    "    print(\"\\nüìä PERFORMANCE SUMMARY\")\n",
    "    print(df_results.groupby(\"k\")[[\"quality\", \"top_score\"]].agg([\"mean\", \"count\"]))\n",
    "    \n",
    "    # Show best/worst\n",
    "    print(\"\\nüèÜ BEST ANSWERS:\")\n",
    "    best = df_results.nlargest(3, \"top_score\")\n",
    "    for _, row in best.iterrows():\n",
    "        print(f\"Q: {row['question'][:60]}...\")\n",
    "        print(f\"  Top source score: {row['top_score']:.3f}\")\n",
    "        print(f\"  Answer: {row['answer'][:80]}...\\n\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag(n_questions=6, k_values=[3, 6])\n",
    "\n",
    "# Experiment: Different K values\n",
    "print(\"\\nüî¨ EXPERIMENT: Impact of K (more context)\")\n",
    "k_experiment = evaluate_rag(n_questions=3, k_values=[1, 3, 6, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç EMBEDDING COMPLETENESS CHECK\n",
      "==================================================\n",
      "Chunks file:   True\n",
      "Embeddings:    True\n",
      "\n",
      "‚úÖ SHAPE MATCH:\n",
      "   Chunks:     36,853\n",
      "   Embeddings: 36,853\n",
      "   ‚úÖ MATCH:    True\n",
      "\n",
      "üîß QUALITY CHECK:\n",
      "   Dimensions:  1536\n",
      "   Total size:  (36853, 1536)\n",
      "   NaN count:   0\n",
      "   Sample norm: 1.000 (should be ~0.8-2.0)\n",
      "\n",
      "üìÇ COVERAGE:\n",
      "   Unique docs:  9,560\n",
      "   Chunks/doc:   3.9 avg\n",
      "\n",
      "üìã SAMPLE:\n",
      "File: 0001144e6d954f94682637e541ad5d7f.md...\n",
      "Text: # 2021-2022 Duffy Fellows\n",
      "\n",
      "**Afrah Bandagi (FCLC 2023)**\n",
      "\n",
      "‚Äú[Supera las fronteras...\n",
      "URL:  https://www.fordham.edu/about/living-the-mission/center-on-religion-and-culture/duffy-fellows-program/past-duffy-fellows/2021-2022-duffy-fellows\n",
      "\n",
      "üéØ STATUS: ‚úÖ 100% COMPLETE\n",
      "RAG Ready: YES\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç EMBEDDING COMPLETENESS CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Check files\n",
    "chunks_file = Path(\"fordham_chunks_final.parquet\")\n",
    "emb_file = Path(\"fordham_embeddings.npy\")\n",
    "\n",
    "print(f\"Chunks file:   {chunks_file.exists()}\")\n",
    "print(f\"Embeddings:    {emb_file.exists()}\")\n",
    "\n",
    "# Load data\n",
    "chunks_df = pd.read_parquet(chunks_file)\n",
    "chunk_embeddings = np.load(emb_file)\n",
    "\n",
    "# 2. CORE CHECK: Shape match\n",
    "n_chunks = len(chunks_df)\n",
    "n_embeds = len(chunk_embeddings)  # Fixed: len() not shape[0]\n",
    "\n",
    "print(f\"\\n‚úÖ SHAPE MATCH:\")\n",
    "print(f\"   Chunks:     {n_chunks:,}\")\n",
    "print(f\"   Embeddings: {n_embeds:,}\")\n",
    "print(f\"   ‚úÖ MATCH:    {n_chunks == n_embeds}\")\n",
    "\n",
    "# 3. Embedding quality\n",
    "print(f\"\\nüîß QUALITY CHECK:\")\n",
    "print(f\"   Dimensions:  {chunk_embeddings.shape[1]}\")\n",
    "print(f\"   Total size:  {chunk_embeddings.shape}\")\n",
    "print(f\"   NaN count:   {np.isnan(chunk_embeddings).sum()}\")\n",
    "\n",
    "# 4. FIXED Norm calculation\n",
    "norm_value = np.linalg.norm(chunk_embeddings[0])\n",
    "print(f\"   Sample norm: {norm_value:.3f} (should be ~0.8-2.0)\")\n",
    "\n",
    "# 5. Document coverage\n",
    "print(f\"\\nüìÇ COVERAGE:\")\n",
    "print(f\"   Unique docs:  {chunks_df['filename'].nunique():,}\")\n",
    "print(f\"   Chunks/doc:   {n_chunks/chunks_df['filename'].nunique():.1f} avg\")\n",
    "\n",
    "# 6. Sample data\n",
    "print(f\"\\nüìã SAMPLE:\")\n",
    "print(f\"File: {chunks_df.iloc[0]['filename'][:40]}...\")\n",
    "print(f\"Text: {chunks_df.iloc[0]['text'][:80]}...\")\n",
    "print(f\"URL:  {chunks_df.iloc[0]['url']}\")\n",
    "\n",
    "# 7. FINAL VERDICT\n",
    "status = \"‚úÖ 100% COMPLETE\" if n_chunks == n_embeds else \"‚ùå RE-EMBED NEEDED\"\n",
    "print(f\"\\nüéØ STATUS: {status}\")\n",
    "print(f\"RAG Ready: {'YES' if n_chunks == n_embeds else 'NO'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. (Optional) Make it an app\n",
    "\n",
    "So far your RAG system lives inside a notebook. That's great for development ‚Äî but nobody is going to use your Jupyter notebook to ask questions about Fordham. Let's turn it into a real web app.\n",
    "\n",
    "> üìö **TERM: Streamlit**  \n",
    "> A Python library that turns plain Python scripts into interactive web apps. You write Python ‚Äî no HTML, CSS, or JavaScript ‚Äî and Streamlit renders it as a web page with inputs, buttons, and formatted output. It's the fastest way to go from \"I have a function\" to \"I have a web app.\"\n",
    "\n",
    "Your task: **create a Streamlit app that lets a user type a question about Fordham and get an answer from your RAG system.**\n",
    "\n",
    "To get started:\n",
    "- Install it: `uv pip install streamlit` \n",
    "- A Streamlit app is just a `.py` file (not a notebook). Create something like `fordham_rag_app.py`\n",
    "- Run it: `streamlit run scripts/fordham_rag_app.py` ‚Äî this opens a browser tab with your app\n",
    "\n",
    "Tips:\n",
    "- Check out the [Streamlit docs](https://docs.streamlit.io/) ‚Äî the \"Get started\" tutorial is very short\n",
    "- Your best bet is to vibecode your way to this. You'll be surprised how fast you can get it up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 14:36:05.685 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2026-02-11 14:36:05.686 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2026-02-11 14:36:05.687 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "StreamlitSecretNotFoundError",
     "evalue": "No secrets found. Valid paths for a secrets.toml file or secret directories are: C:\\Users\\Manikandan\\.streamlit\\secrets.toml, d:\\Spring 2026\\RAG and Context Engineering\\HW\\.streamlit\\secrets.toml",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStreamlitSecretNotFoundError\u001b[39m              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m chunks_df, chunk_embeddings = load_data()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Config\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m client = OpenAI(api_key=\u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43msecrets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOPENAI_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     19\u001b[39m EMBED_MODEL = \u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m GEN_MODEL = \u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\streamlit\\runtime\\secrets.py:473\u001b[39m, in \u001b[36mSecrets.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the value with the given key. If no such key\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[33;03mexists, raise a KeyError.\u001b[39;00m\n\u001b[32m    469\u001b[39m \n\u001b[32m    470\u001b[39m \u001b[33;03mThread-safe.\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[key]\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Mapping):\n\u001b[32m    475\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\streamlit\\runtime\\secrets.py:375\u001b[39m, in \u001b[36mSecrets._parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found_secrets_file:\n\u001b[32m    370\u001b[39m     error_msg = (\n\u001b[32m    371\u001b[39m         secret_error_messages_singleton.get_no_secrets_found_message(\n\u001b[32m    372\u001b[39m             file_paths\n\u001b[32m    373\u001b[39m         )\n\u001b[32m    374\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StreamlitSecretNotFoundError(error_msg)\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m secrets.items():\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m._maybe_set_environment_variable(k, v)\n",
      "\u001b[31mStreamlitSecretNotFoundError\u001b[39m: No secrets found. Valid paths for a secrets.toml file or secret directories are: C:\\Users\\Manikandan\\.streamlit\\secrets.toml, d:\\Spring 2026\\RAG and Context Engineering\\HW\\.streamlit\\secrets.toml"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.express as px\n",
    "\n",
    "# Load your data\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    chunks_df = pd.read_parquet(\"fordham_chunks_final.parquet\")\n",
    "    chunk_embeddings = np.load(\"fordham_embeddings.npy\")\n",
    "    return chunks_df, chunk_embeddings\n",
    "\n",
    "chunks_df, chunk_embeddings = load_data()\n",
    "\n",
    "# Config\n",
    "client = OpenAI(api_key=st.secrets[\"OPENAI_API_KEY\"])\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "GEN_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Your functions (same as notebook)\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=query[:4000])\n",
    "    return np.array(resp.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "def retrieve_top_k(query: str, k: int = 6) -> pd.DataFrame:\n",
    "    q_emb = embed_query(query)\n",
    "    sims = cosine_similarity(q_emb.reshape(1, -1), chunk_embeddings)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:k]\n",
    "    results = chunks_df.iloc[top_idx].copy()\n",
    "    results[\"score\"] = sims[top_idx]\n",
    "    return results.sort_values(\"score\", ascending=False)\n",
    "\n",
    "def rag(question: str, k: int = 6) -> dict:\n",
    "    retrieved = retrieve_top_k(question, k)\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"[Source: {row['url']}]\\n{row['text'][:800]}...\" \n",
    "        for _, row in retrieved.iterrows()\n",
    "    ])\n",
    "    \n",
    "    prompt = \"\"\"Using ONLY this Fordham University context, answer the question.\n",
    "\n",
    "CONTEXT:\n",
    "{}\n",
    "QUESTION: {}\n",
    "\n",
    "Answer briefly using only the context above. If not in context, say \"Not found in Fordham docs.\" \"\"\".format(context, question)\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=GEN_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": resp.choices[0].message.content.strip(),\n",
    "        \"sources\": retrieved[[\"url\", \"filename\", \"score\"]].to_dict(\"records\")\n",
    "    }\n",
    "\n",
    "# === STREAMLIT UI ===\n",
    "st.set_page_config(page_title=\"Fordham RAG\", layout=\"wide\")\n",
    "st.title(\"üöÄ Fordham University RAG Assistant\")\n",
    "st.markdown(\"Ask questions about Fordham programs, admissions, campus life... powered by 36K+ Fordham web pages\")\n",
    "\n",
    "# Sidebar controls\n",
    "st.sidebar.header(\"‚öôÔ∏è Settings\")\n",
    "k_value = st.sidebar.slider(\"Context chunks (K)\", 3, 15, 6)\n",
    "show_sources = st.sidebar.checkbox(\"Show sources\", True)\n",
    "\n",
    "# Main chat interface\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "        if message[\"role\"] == \"assistant\" and \"sources\" in message:\n",
    "            with st.expander(\"üìö Sources\"):\n",
    "                for i, source in enumerate(message[\"sources\"][:5]):\n",
    "                    st.markdown(f\"**{i+1}.** [{source['filename']}]({source['url']}) (score: {source['score']:.3f})\")\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask about Fordham...\"):\n",
    "    # Add user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Searching Fordham docs...\"):\n",
    "            result = rag(prompt, k=k_value)\n",
    "        \n",
    "        st.markdown(result[\"answer\"])\n",
    "        \n",
    "        # Store full result for sources\n",
    "        full_result = {\"role\": \"assistant\", \"content\": result[\"answer\"], \n",
    "                      \"sources\": result[\"sources\"]}\n",
    "        st.session_state.messages.append(full_result)\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"**Built with:** 36,853 Fordham web page chunks | OpenAI GPT-4o-mini | Semantic search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What You Built\n",
    "\n",
    "| Step | What You Did | What It Does |\n",
    "|------|-------------|-------------|\n",
    "| **Load** | Read 9,500+ Fordham web pages | Get raw content |\n",
    "| **Chunk** | Split pages into smaller pieces | Make content searchable and promptable |\n",
    "| **Embed** | Turn chunks into vectors | Enable semantic search |\n",
    "| **Retrieve** | Find relevant chunks for a question | The **R** in RAG |\n",
    "| **Generate** | Ask an LLM to answer using the chunks | The **G** in RAG |\n",
    "| **RAG** | Wire it all together | Question in, answer out |\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "RAG is one of the most common patterns in AI engineering today. What you built here is the same core architecture behind tools like ChatGPT with search, Perplexity, enterprise Q&A bots, and more. The details get more sophisticated (vector databases, reranking, query rewriting, evaluation) but the pattern is the same:\n",
    "\n",
    "**Find relevant stuff ‚Üí give it to an LLM ‚Üí get an answer.**\n",
    "\n",
    "You can just build things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
